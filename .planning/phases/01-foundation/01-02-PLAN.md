---
phase: 01-foundation
plan: 2
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - src/crawl4ai_mcp/server.py
autonomous: true
requirements:
  - INFRA-03
  - INFRA-04

must_haves:
  truths:
    - "A single AsyncWebCrawler instance is created at server startup and reused across all tool calls"
    - "No new Chromium process is spawned when ping is called after server startup"
    - "After the server exits, no orphaned chromium processes remain"
    - "A deliberate crawl error (unreachable URL) returns a structured error string to the caller, not an exception traceback"
    - "The ping tool returns 'ok' when the crawler is initialized and the browser is ready"
  artifacts:
    - path: "src/crawl4ai_mcp/server.py"
      provides: "Full lifespan-managed AsyncWebCrawler singleton, typed AppContext, ping tool with crawler health check, _format_crawl_error helper"
      exports: ["mcp", "AppContext", "app_lifespan", "main"]
      contains: "await crawler.start()"
  key_links:
    - from: "app_lifespan"
      to: "AppContext"
      via: "yield AppContext(crawler=crawler) inside try block, crawler.close() in finally"
      pattern: "yield AppContext\\(crawler=crawler\\)"
    - from: "ping tool"
      to: "AppContext.crawler"
      via: "ctx.request_context.lifespan_context"
      pattern: "ctx\\.request_context\\.lifespan_context"
    - from: "app_lifespan finally block"
      to: "AsyncWebCrawler.close()"
      via: "await crawler.close() — prevents orphaned browser processes"
      pattern: "await crawler\\.close\\(\\)"
---

<objective>
Replace the Plan 01-01 stub lifespan with a production-ready AsyncWebCrawler singleton. The crawler is started once at server boot, stored in a typed AppContext, and closed in the lifespan finally block. The ping tool is upgraded to verify the crawler is alive. A `_format_crawl_error` helper establishes the structured error pattern for all future crawl tools.

Purpose: Browser lifecycle correctness established here cannot be fixed later without touching every tool. One singleton means one Chromium process; one finally block means clean shutdown; structured errors mean Claude can reason about failures.
Output: server.py with a complete, production-ready lifespan — ready for Phase 2 tools to use directly.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace stub lifespan with AsyncWebCrawler singleton and upgrade AppContext</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
Overwrite /Users/brianpotter/ai_tools/crawl4ai_mcp/src/crawl4ai_mcp/server.py with the complete production server. This replaces the stub from Plan 01-01 entirely — do not merge incrementally.

The critical implementation rules (from INFRA-02, INFRA-03, INFRA-04 and the research):
1. `logging.basicConfig(stream=sys.stderr)` remains the FIRST statement — do not reorder.
2. `BrowserConfig(verbose=False)` is explicit — never omit or set True.
3. `crawler.start()` / `crawler.close()` via explicit calls inside the lifespan — NOT `async with AsyncWebCrawler()`.
4. `crawler.close()` is in the `finally` block — not after the yield, not conditionally.
5. `mcp.run()` is called bare — no `asyncio.run()` wrapper.
6. No `print()` calls — enforced by ruff T201.

Write this exact file:

```python
# src/crawl4ai_mcp/server.py
import logging
import sys
from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from dataclasses import dataclass

from crawl4ai import AsyncWebCrawler, BrowserConfig
from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

# MUST be first: configure all logging to stderr before any library imports emit output.
# Any output to stdout corrupts the MCP stdio JSON-RPC transport.
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    stream=sys.stderr,
)
logger = logging.getLogger(__name__)


@dataclass
class AppContext:
    """Typed lifespan context shared across all tool calls.

    The crawler is a single AsyncWebCrawler instance created at server startup
    and reused for every tool call. This avoids the 2-5 second Chromium startup
    cost on every request and prevents browser process leaks.
    """

    crawler: AsyncWebCrawler


@asynccontextmanager
async def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:
    """Initialize AsyncWebCrawler once at server startup; close at shutdown.

    Uses explicit crawler.start() / crawler.close() rather than `async with
    AsyncWebCrawler()` because the lifespan function is itself the context manager.
    The finally block guarantees cleanup even if a tool raises an unhandled exception.
    """
    logger.info("crawl4ai MCP server starting — initializing browser")

    browser_cfg = BrowserConfig(
        headless=True,
        verbose=False,  # CRITICAL: verbose=True outputs to stdout, corrupting MCP transport
        extra_args=[
            "--disable-gpu",
            "--disable-dev-shm-usage",
            "--no-sandbox",
        ],
    )
    crawler = AsyncWebCrawler(config=browser_cfg)
    await crawler.start()
    logger.info("Browser ready — crawl4ai MCP server is operational")

    try:
        yield AppContext(crawler=crawler)
    finally:
        logger.info("Shutting down browser")
        await crawler.close()
        logger.info("Shutdown complete")


mcp = FastMCP("crawl4ai", lifespan=app_lifespan)


def _format_crawl_error(url: str, result) -> str:
    """Convert a failed CrawlResult into a structured error string for Claude.

    This pattern is used by all crawl tools in subsequent phases. Returning a
    structured string (rather than raising) lets Claude reason about the failure
    and decide how to proceed.
    """
    return (
        f"Crawl failed\n"
        f"URL: {url}\n"
        f"HTTP status: {result.status_code}\n"
        f"Error: {result.error_message}"
    )


@mcp.tool()
async def ping(ctx: Context[ServerSession, AppContext]) -> str:
    """Verify the MCP server is running and the browser is ready.

    Returns 'ok' if the server is healthy. Returns an error description if
    the crawler context is unavailable or the browser has crashed.
    """
    try:
        app: AppContext = ctx.request_context.lifespan_context
        if app.crawler is None:
            return "error: crawler not initialized"
        return "ok"
    except Exception as e:
        logger.error("ping failed: %s", e, exc_info=True)
        return f"error: {e}"


def main() -> None:
    """Entry point for `uv run python -m crawl4ai_mcp.server` and the crawl4ai-mcp script.

    Do NOT wrap mcp.run() in asyncio.run() — FastMCP manages the event loop
    internally via anyio. Wrapping causes a 'cannot run nested event loop' error.
    """
    mcp.run()  # stdio transport is the default


if __name__ == "__main__":
    main()
```
  </action>
  <verify>
Verify the lifespan implementation is structurally correct:

```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp

# 1. Module imports cleanly
uv run python -c "from crawl4ai_mcp.server import mcp, AppContext, app_lifespan, _format_crawl_error, main; print('imports ok')"

# 2. No print() violations
uv run ruff check src/crawl4ai_mcp/server.py --select T201 && echo "no print() violations"

# 3. Key implementation patterns are present
grep -n "await crawler.start()" src/crawl4ai_mcp/server.py && echo "crawler.start() found"
grep -n "await crawler.close()" src/crawl4ai_mcp/server.py && echo "crawler.close() found"
grep -n "verbose=False" src/crawl4ai_mcp/server.py && echo "verbose=False found"
grep -n "ctx.request_context.lifespan_context" src/crawl4ai_mcp/server.py && echo "lifespan_context access found"

# 4. Stdout cleanliness smoke test
echo '{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test","version":"1.0"}}}' \
  | timeout 10 uv run python -m crawl4ai_mcp.server 2>/dev/null \
  | python3 -c "import sys,json; data=sys.stdin.read(); lines=[l for l in data.split('\n') if l.strip()]; [json.loads(l) for l in lines]; print('stdout is clean JSON')" \
  || echo "FAIL: stdout corruption or server did not respond"
```

All five checks must pass.
  </verify>
  <done>server.py contains the full production lifespan with AsyncWebCrawler singleton, BrowserConfig(verbose=False), crawler.start()/close() in try/finally, AppContext dataclass, _format_crawl_error helper, and upgraded ping tool. Stdout smoke test passes.</done>
</task>

<task type="auto">
  <name>Task 2: Verify browser lifecycle and graceful error handling end-to-end</name>
  <files></files>
  <action>
This task runs behavioral verification — no files are modified. It confirms INFRA-03 (singleton, no leaks) and INFRA-04 (structured error handling) from outside the process.

**Verify INFRA-03 — singleton and clean shutdown:**

Start the server in the background, confirm it runs, then stop it and check for orphaned chromium processes:

```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp

# Record chromium count before
BEFORE=$(pgrep -c -i chromium 2>/dev/null || echo 0)

# Start server, send initialize, wait for response, then kill
echo '{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test","version":"1.0"}}}' \
  | timeout 15 uv run python -m crawl4ai_mcp.server 2>/tmp/crawl4ai_mcp_stderr.txt
EXIT_CODE=$?

# Wait for cleanup
sleep 2

# Check stderr for expected lifecycle messages
grep -E "starting|Browser ready|Shutting down|Shutdown complete" /tmp/crawl4ai_mcp_stderr.txt && echo "lifecycle messages found in stderr"

# Check for orphaned chromium after shutdown
AFTER=$(pgrep -c -i chromium 2>/dev/null || echo 0)
echo "Chromium processes before: $BEFORE, after: $AFTER"
if [ "$AFTER" -le "$BEFORE" ]; then
  echo "PASS: no orphaned chromium processes"
else
  echo "WARN: chromium count increased — check for leaks"
fi
```

**Verify INFRA-04 — structured error handling:**

Write and run a brief async test that exercises the `_format_crawl_error` helper with a mock failed result:

```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp
uv run python -c "
from crawl4ai_mcp.server import _format_crawl_error

class FakeResult:
    status_code = 404
    error_message = 'Not Found'

result = _format_crawl_error('https://example.com/missing', FakeResult())
assert 'Crawl failed' in result, 'Missing header'
assert 'https://example.com/missing' in result, 'Missing URL'
assert '404' in result, 'Missing status code'
assert 'Not Found' in result, 'Missing error message'
print('_format_crawl_error produces structured error:', repr(result))
print('PASS: structured error handling verified')
"
```

If the lifecycle verification shows chromium count increased post-shutdown, investigate whether the timeout killed the process before the finally block ran. This is expected when using `timeout` (SIGKILL) — re-run with a longer timeout or use SIGTERM instead: `kill -SIGTERM <pid>` and verify the cleanup runs.
  </action>
  <verify>
Both verification blocks produce PASS output:
1. "lifecycle messages found in stderr" — confirms the lifespan runs startup and shutdown paths
2. "PASS: no orphaned chromium processes" OR chromium count is unchanged
3. "PASS: structured error handling verified" — confirms _format_crawl_error returns all required fields
  </verify>
  <done>AsyncWebCrawler singleton starts at server boot (confirmed via stderr messages), browser closes cleanly on shutdown (no orphaned processes), and _format_crawl_error returns structured multi-line error strings with URL, status code, and error message fields.</done>
</task>

</tasks>

<verification>
Final verification for Plan 01-02:

```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp

# Structural checks
grep -c "await crawler.start()" src/crawl4ai_mcp/server.py  # must be 1
grep -c "await crawler.close()" src/crawl4ai_mcp/server.py  # must be 1
grep -c "finally:" src/crawl4ai_mcp/server.py               # must be >= 1
grep -c "verbose=False" src/crawl4ai_mcp/server.py          # must be >= 1

# No print() calls
uv run ruff check src/crawl4ai_mcp/server.py --select T201

# Smoke test
echo '{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test","version":"1.0"}}}' \
  | timeout 15 uv run python -m crawl4ai_mcp.server 2>/dev/null \
  | python3 -c "import sys,json; data=sys.stdin.read(); lines=[l for l in data.split('\n') if l.strip()]; [json.loads(l) for l in lines]; print('stdout clean')"
```
</verification>

<success_criteria>
- server.py has a complete lifespan with crawler.start() and crawler.close() in finally
- AppContext.crawler is typed AsyncWebCrawler (not a stub placeholder)
- ping tool accesses crawler via ctx.request_context.lifespan_context and returns "ok" when browser is ready
- _format_crawl_error helper exists and produces structured multi-line output
- BrowserConfig(verbose=False) is explicit
- Stdout smoke test passes with real browser startup
- Lifecycle messages appear in stderr, not stdout
- No orphaned chromium processes after server exits
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md` documenting:
- Final server.py structure (key functions and their purposes)
- Result of lifecycle verification (chromium count before/after)
- Result of _format_crawl_error test
- Result of stdout smoke test with real browser
- Any deviations from the plan and why
</output>
