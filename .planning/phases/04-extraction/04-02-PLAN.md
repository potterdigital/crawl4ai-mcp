---
phase: 04-extraction
plan: 02
type: execute
wave: 2
depends_on: ["01"]
files_modified:
  - src/crawl4ai_mcp/server.py
  - tests/test_extraction_css.py
autonomous: true
requirements:
  - EXTR-02
  - EXTR-03

must_haves:
  truths:
    - "Claude can call extract_css with a baseSelector and field definitions and receive structured JSON"
    - "extract_css makes no LLM calls and incurs no API cost"
    - "crawl_url never triggers CSS extraction — extract_css is a separate entry point"
    - "Empty extraction results return a meaningful message, not a silent empty string"
  artifacts:
    - path: "src/crawl4ai_mcp/server.py"
      provides: "extract_css tool function with @mcp.tool() decorator"
      contains: "async def extract_css"
    - path: "tests/test_extraction_css.py"
      provides: "Unit tests for extract_css registration and docstring"
  key_links:
    - from: "src/crawl4ai_mcp/server.py extract_css"
      to: "crawl4ai.JsonCssExtractionStrategy"
      via: "strategy construction with schema dict"
      pattern: "JsonCssExtractionStrategy.*schema.*verbose=False"
    - from: "src/crawl4ai_mcp/server.py extract_css"
      to: "AppContext.crawler"
      via: "ctx.request_context.lifespan_context"
      pattern: "ctx\\.request_context\\.lifespan_context"
---

<objective>
Add the `extract_css` MCP tool for deterministic CSS-selector-based JSON extraction from web pages.

Purpose: Gives Claude a cost-free, deterministic extraction tool using CSS selectors. No LLM call, no API key needed (EXTR-02). Combined with plan 04-01's `extract_structured`, this completes the EXTR-03 separation: `crawl_url` never triggers any extraction strategy.

Output: Working `extract_css` tool in server.py, unit tests for registration and behavior.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-extraction/04-RESEARCH.md
@.planning/phases/04-extraction/04-01-SUMMARY.md
@src/crawl4ai_mcp/server.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add extract_css tool to server.py</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
Add `JsonCssExtractionStrategy` to the existing crawl4ai import line in server.py. The import line should now include (along with whatever 04-01 added):
```python
from crawl4ai import ..., JsonCssExtractionStrategy
```

Add the `extract_css` tool function after `extract_structured` (which was added by plan 04-01). Signature:

```python
@mcp.tool()
async def extract_css(
    url: str,
    schema: dict,
    css_selector: str | None = None,
    wait_for: str | None = None,
    js_code: str | None = None,
    page_timeout: int = 60,
    ctx: Context[ServerSession, AppContext] = None,
) -> str:
```

The docstring should clearly state this is cost-free and explain the schema format:
```
"""Extract structured JSON from a page using CSS selectors (no LLM, no cost).

Uses crawl4ai's JsonCssExtractionStrategy for deterministic, repeatable
extraction. No LLM API call is made — this tool is completely free to use.

Args:
    url: The URL to crawl and extract data from.

    schema: Extraction schema dict defining what to extract. Must contain:
        - "name": A label for the extraction (e.g. "Products")
        - "baseSelector": CSS selector matching each repeating item
          (e.g. "div.product-card")
        - "fields": List of field definitions, each with:
          - "name": Field name in output JSON
          - "selector": CSS selector relative to baseSelector
          - "type": One of "text", "attribute", "html", "regex",
            "list", "nested", "nested_list"
          - "attribute": Required when type is "attribute" (e.g. "href", "src")
          - "transform": Optional, e.g. "strip", "lowercase"
          - "default": Optional default value if selector matches nothing
          - "fields": Required for "nested"/"nested_list"/"list" types
            (recursive field definitions)

        Example:
        {
            "name": "Products",
            "baseSelector": "div.product",
            "fields": [
                {"name": "title", "selector": "h2", "type": "text"},
                {"name": "price", "selector": ".price", "type": "text"},
                {"name": "url", "selector": "a", "type": "attribute",
                 "attribute": "href"}
            ]
        }

    css_selector: Restrict extraction scope to elements matching this
        CSS selector before applying the extraction schema.

    wait_for: Wait condition before extraction (CSS: "css:#el",
        JS: "js:() => expr"). Useful for dynamically loaded content.

    js_code: JavaScript to execute after page load, before extraction.
        Use to trigger lazy loading or expand collapsed sections.

    page_timeout: Page load timeout in seconds (default 60).
"""
```

Implementation:
1. Log the call: `logger.info("extract_css: %s", url)`.
2. Create `JsonCssExtractionStrategy(schema, verbose=False)`. The `verbose=False` is CRITICAL.
3. Construct `CrawlerRunConfig` directly (NOT via `build_run_config`):
   ```python
   run_cfg = CrawlerRunConfig(
       extraction_strategy=strategy,
       page_timeout=page_timeout * 1000,
       verbose=False,
   )
   ```
4. Conditionally set optional params on `run_cfg`: `css_selector`, `wait_for`, `js_code` (only if not None).
5. Get `app: AppContext = ctx.request_context.lifespan_context`.
6. Call `result = await _crawl_with_overrides(app.crawler, url, run_cfg)`.
7. If `not result.success`, return `_format_crawl_error(url, result)`.
8. Check `result.extracted_content` — if None or empty string or equals `"[]"`, return:
   ```
   "No data extracted\n"
   "URL: {url}\n"
   "The CSS selectors in the schema did not match any elements on the page.\n"
   "Verify that baseSelector and field selectors are correct for this page's HTML structure."
   ```
9. Return `result.extracted_content`.

Critical constraints:
- `verbose=False` on BOTH the strategy AND the CrawlerRunConfig.
- Do NOT route through `build_run_config` — extraction tools construct CrawlerRunConfig directly.
- Do NOT import or use LLMConfig, LLMExtractionStrategy, or any LLM-related code in this tool.
- The `schema` parameter is the only required structured input — it IS the extraction definition.
  </action>
  <verify>
Run `uv run ruff check src/crawl4ai_mcp/server.py` — no errors.
Run `uv run python -c "from crawl4ai_mcp.server import extract_css; print('import ok')"` — confirms the tool exists.
  </verify>
  <done>
`extract_css` tool exists in server.py with @mcp.tool() decorator, clear "no LLM, no cost" messaging in docstring, comprehensive schema format documentation in Args, verbose=False on both strategy and config, and meaningful empty-result handling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for extract_css registration and EXTR-03 enforcement</name>
  <files>tests/test_extraction_css.py</files>
  <action>
Create `tests/test_extraction_css.py` with the following test classes:

**TestExtractCssRegistration** — tests that the tool is properly registered:
1. `test_tool_registered` — import `mcp` from `crawl4ai_mcp.server`, verify `extract_css` is in the tool list.
2. `test_docstring_no_llm_no_cost` — import `extract_css`, assert `__doc__` contains "no LLM" and "no cost".
3. `test_no_provider_parameter` — inspect `extract_css` signature (use `inspect.signature`), assert "provider" is NOT in its parameters — confirms no LLM config leaks into the CSS tool.

**TestExtr03Enforcement** — tests that crawl_url does NOT trigger extraction:
1. `test_crawl_url_has_no_extraction_strategy_param` — import `crawl_url` from `crawl4ai_mcp.server`, use `inspect.signature` to verify "extraction_strategy" is NOT in its parameters.
2. `test_crawl_url_has_no_schema_param` — same, verify "schema" is NOT in `crawl_url` parameters.
3. `test_extraction_tools_are_separate_functions` — import `crawl_url`, `extract_structured`, and `extract_css`. Assert they are distinct function objects (`crawl_url is not extract_css`, `extract_structured is not extract_css`).

Import `inspect` for signature introspection.
Import from `crawl4ai_mcp.server`: `extract_css`, `extract_structured`, `crawl_url`.
  </action>
  <verify>
Run `uv run pytest tests/test_extraction_css.py -v` — all tests pass.
Run `uv run pytest` — all tests pass (full regression).
  </verify>
  <done>
`tests/test_extraction_css.py` exists with tests for tool registration, no-LLM docstring, no provider parameter, and EXTR-03 enforcement (crawl_url has no extraction params, extraction tools are separate functions). All tests pass. Existing tests still pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run ruff check src/` — clean
2. `uv run pytest` — all tests pass (profiles + extraction + extraction_css)
3. `grep "no LLM" src/crawl4ai_mcp/server.py` — appears in extract_css docstring
4. `grep "verbose=False" src/crawl4ai_mcp/server.py` — appears in JsonCssExtractionStrategy and CrawlerRunConfig construction
5. `grep -c "extraction_strategy" src/crawl4ai_mcp/server.py` — appears only in extract_structured and extract_css, NOT in crawl_url
</verification>

<success_criteria>
- extract_css tool exists and is registered as an MCP tool
- Tool docstring clearly states "no LLM, no cost"
- Schema format documented with example in docstring
- verbose=False on both strategy and config
- crawl_url has no extraction_strategy, schema, or provider parameters (EXTR-03)
- Empty extraction results return informative message
- All tests pass (new + existing)
</success_criteria>

<output>
After completion, create `.planning/phases/04-extraction/04-02-SUMMARY.md`
</output>
