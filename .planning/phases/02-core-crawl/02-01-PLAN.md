---
phase: 02-core-crawl
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/crawl4ai_mcp/server.py
autonomous: true
requirements: [CORE-01, CORE-02, CORE-03, CORE-04, CORE-05]

must_haves:
  truths:
    - "crawl_url tool exists and is registered on the MCP server"
    - "A URL crawl returns fit_markdown content (PruningContentFilter applied by default)"
    - "Cache mode, CSS scope selectors, JS code, wait_for, headers, cookies, user_agent, and page_timeout are all accepted as optional parameters"
    - "CrawlerRunConfig always has verbose=False to prevent MCP stdout corruption"
    - "Per-request headers and cookies are injected via before_goto/on_page_context_created Playwright hooks and always cleared in a finally block"
    - "Failed crawls return a structured error string (not raise) using _format_crawl_error"
    - "All imports needed by crawl_url are present in server.py"
  artifacts:
    - path: "src/crawl4ai_mcp/server.py"
      provides: "crawl_url tool, _build_run_config helper, _crawl_with_overrides helper"
      contains: "async def crawl_url"
    - path: "src/crawl4ai_mcp/server.py"
      provides: "_build_run_config helper building CrawlerRunConfig with PruningContentFilter"
      contains: "PruningContentFilter"
    - path: "src/crawl4ai_mcp/server.py"
      provides: "_crawl_with_overrides helper with hook-based header/cookie injection"
      contains: "before_goto"
  key_links:
    - from: "crawl_url"
      to: "_build_run_config"
      via: "direct function call"
      pattern: "_build_run_config\\("
    - from: "crawl_url"
      to: "_crawl_with_overrides"
      via: "direct function call"
      pattern: "_crawl_with_overrides\\("
    - from: "_crawl_with_overrides"
      to: "crawler.arun"
      via: "await"
      pattern: "await crawler\\.arun"
    - from: "crawl_url"
      to: "_format_crawl_error"
      via: "return on failure"
      pattern: "return _format_crawl_error"
---

<objective>
Implement the crawl_url MCP tool — the core deliverable of Phase 2 — adding it directly to server.py alongside two private helper functions.

Purpose: Give Claude Code the ability to crawl any URL and receive clean, filtered markdown content with full control over JS rendering, request parameters, cache behavior, and CSS content scoping.
Output: A fully functional crawl_url tool registered on the MCP server, backed by _build_run_config and _crawl_with_overrides helpers.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-crawl/02-RESEARCH.md
@src/crawl4ai_mcp/server.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add imports and helper functions to server.py</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
Add the following imports after the existing `from crawl4ai import AsyncWebCrawler, BrowserConfig` line:

```python
from crawl4ai import CacheMode, CrawlerRunConfig, DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
```

Then add two private helper functions after the existing `_format_crawl_error` function and before the `@mcp.tool()` ping definition:

**_build_run_config**: Builds a CrawlerRunConfig with PruningContentFilter by default. Signature:
```python
def _build_run_config(
    cache_mode: CacheMode = CacheMode.ENABLED,
    css_selector: str | None = None,
    excluded_selector: str | None = None,
    word_count_threshold: int = 10,
    wait_for: str | None = None,
    js_code: str | list[str] | None = None,
    user_agent: str | None = None,
    page_timeout: int = 60000,
) -> CrawlerRunConfig:
```

Body: Create `DefaultMarkdownGenerator(content_filter=PruningContentFilter(threshold=0.48, threshold_type="fixed", min_word_threshold=word_count_threshold))`. Return `CrawlerRunConfig(markdown_generator=md_gen, cache_mode=cache_mode, css_selector=css_selector, excluded_selector=excluded_selector, wait_for=wait_for, js_code=js_code, user_agent=user_agent, page_timeout=page_timeout, verbose=False)`. The `verbose=False` is CRITICAL — omitting it defaults to True and writes to stdout, corrupting the MCP transport.

**_crawl_with_overrides**: Injects per-request headers and cookies via Playwright hooks, runs arun, then clears all hooks in a finally block. Signature:
```python
async def _crawl_with_overrides(
    crawler: AsyncWebCrawler,
    url: str,
    config: CrawlerRunConfig,
    headers: dict | None = None,
    cookies: list | None = None,
):
```

Body: Get `strategy = crawler.crawler_strategy`. If `headers` is truthy, define `async def before_goto(page, context, url, config, **kwargs): await page.set_extra_http_headers(headers)` and call `strategy.set_hook("before_goto", before_goto)`. If `cookies` is truthy, define `async def on_page_context_created(page, context, **kwargs): await context.add_cookies(cookies)` and call `strategy.set_hook("on_page_context_created", on_page_context_created)`. Then:
```python
    try:
        return await crawler.arun(url=url, config=config)
    finally:
        if headers:
            strategy.set_hook("before_goto", None)
        if cookies:
            strategy.set_hook("on_page_context_created", None)
```
The try/finally is REQUIRED — hooks must be cleared even if arun raises, to prevent hook leakage into subsequent calls.

Add a docstring to each helper explaining its purpose. Do NOT use `print()` anywhere.
  </action>
  <verify>
Run `uv run ruff check src/` from /Users/brianpotter/ai_tools/crawl4ai_mcp and confirm zero errors (T201 print check + standard lint). Also confirm both function names appear: `grep -n "_build_run_config\|_crawl_with_overrides" src/crawl4ai_mcp/server.py`
  </verify>
  <done>ruff reports no errors; both helper functions are present in server.py with correct signatures; verbose=False is explicit in _build_run_config; try/finally hook cleanup is present in _crawl_with_overrides</done>
</task>

<task type="auto">
  <name>Task 2: Add crawl_url tool</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
Add the `crawl_url` tool after the existing `ping` tool (but before the `main()` function). Decorate with `@mcp.tool()`.

Full signature:
```python
@mcp.tool()
async def crawl_url(
    url: str,
    cache_mode: str = "enabled",
    css_selector: str | None = None,
    excluded_selector: str | None = None,
    wait_for: str | None = None,
    js_code: str | None = None,
    user_agent: str | None = None,
    headers: dict | None = None,
    cookies: list | None = None,
    page_timeout: int = 60,
    word_count_threshold: int = 10,
    ctx: Context[ServerSession, AppContext] = None,
) -> str:
```

Write a comprehensive docstring (used by Claude to understand the tool). Cover:
- What the tool does (crawls a URL, returns fit_markdown by default)
- `cache_mode`: accepted values are "enabled" (use cache if available), "bypass" (always fetch fresh, don't cache), "disabled" (fetch fresh, no read/write), "read_only" (cached only, fail if missing), "write_only" (fetch fresh, overwrite cache)
- `css_selector`: restrict extraction to elements matching this CSS selector (include scope)
- `excluded_selector`: exclude elements matching this CSS selector from extraction (exclude noise like navbars, footers)
- `wait_for`: wait until a CSS selector or JS condition is met before extracting — format: "css:#element" or "js:() => condition"
- `js_code`: JavaScript to execute after page load before extraction (e.g., scroll, click)
- `user_agent`: override the browser User-Agent for this request
- `headers`: dict of custom HTTP headers to send with the request (e.g., {"Authorization": "Bearer token"})
- `cookies`: list of cookie dicts sent with the request, each with required keys: name, value, domain (e.g., [{"name": "session", "value": "abc", "domain": "example.com"}])
- `page_timeout`: how long to wait for page load in seconds (default 60)
- `word_count_threshold`: minimum word count for a content block to survive PruningContentFilter (default 10)

Body:
1. Map `cache_mode` string to CacheMode enum:
```python
    _CACHE_MAP = {
        "enabled": CacheMode.ENABLED,
        "bypass": CacheMode.BYPASS,
        "disabled": CacheMode.DISABLED,
        "read_only": CacheMode.READ_ONLY,
        "write_only": CacheMode.WRITE_ONLY,
    }
    resolved_cache = _CACHE_MAP.get(cache_mode, CacheMode.ENABLED)
    if cache_mode not in _CACHE_MAP:
        logger.warning("Unknown cache_mode %r — defaulting to 'enabled'", cache_mode)
```

2. Call `_build_run_config(...)` passing all relevant params (convert page_timeout to ms: `page_timeout * 1000`).

3. Get `app: AppContext = ctx.request_context.lifespan_context`.

4. Call `result = await _crawl_with_overrides(app.crawler, url, run_cfg, headers, cookies)`.

5. Error check: `if not result.success: return _format_crawl_error(url, result)`.

6. Extract content: `md = result.markdown; content = (md.fit_markdown or md.raw_markdown) if md else ""`. Return `content`.

Log at INFO level: `logger.info("crawl_url: %s (cache=%s)", url, cache_mode)` at the start of the function body (after the cache mapping).

Do NOT use `print()`. Do NOT set `verbose=True` anywhere.
  </action>
  <verify>
Run `uv run ruff check src/` — must report zero errors.
Run `uv run python -c "from crawl4ai_mcp.server import crawl_url; print('import ok')"` from /Users/brianpotter/ai_tools/crawl4ai_mcp — must print "import ok" with no errors.
  </verify>
  <done>ruff passes; crawl_url is importable; tool has all 11 parameters (url, cache_mode, css_selector, excluded_selector, wait_for, js_code, user_agent, headers, cookies, page_timeout, word_count_threshold) plus ctx; docstring covers all params</done>
</task>

</tasks>

<verification>
After both tasks:
1. `uv run ruff check src/` — zero errors
2. `uv run python -c "from crawl4ai_mcp.server import crawl_url, _build_run_config, _crawl_with_overrides; print('ok')"` — prints "ok"
3. `grep -c "verbose=False" src/crawl4ai_mcp/server.py` — must show at least 2 (BrowserConfig and CrawlerRunConfig)
4. `grep -n "before_goto\|on_page_context_created" src/crawl4ai_mcp/server.py` — must show both hooks defined and cleared
</verification>

<success_criteria>
- crawl_url tool registered on the MCP server, all 5 Phase 2 requirements addressed in a single tool
- _build_run_config helper produces CrawlerRunConfig with verbose=False and PruningContentFilter
- _crawl_with_overrides helper injects headers/cookies via hooks and clears them in finally
- ruff lint passes (no T201 print violations, no other errors)
- All imports resolve (CacheMode, CrawlerRunConfig, DefaultMarkdownGenerator, PruningContentFilter)
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-crawl/02-01-SUMMARY.md`
</output>
