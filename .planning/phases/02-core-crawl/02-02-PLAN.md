---
phase: 02-core-crawl
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - README.md
autonomous: true
requirements: [CORE-01, CORE-02, CORE-03, CORE-04, CORE-05]

must_haves:
  truths:
    - "A real HTTP crawl via MCP returns non-empty fit_markdown content"
    - "The README tool table lists crawl_url with its parameters documented"
    - "The smoke test confirms the full MCP stack is operational (not just import-level)"
  artifacts:
    - path: "README.md"
      provides: "crawl_url tool documentation with parameters and usage examples"
      contains: "crawl_url"
  key_links:
    - from: "smoke test"
      to: "crawl_url"
      via: "MCP JSON-RPC call or direct Python invocation"
      pattern: "crawl_url"
    - from: "README.md"
      to: "crawl_url"
      via: "tool table entry"
      pattern: "crawl_url"
---

<objective>
Verify the crawl_url implementation with a real end-to-end crawl and update the README to document the new tool.

Purpose: Confirm the full MCP stack works (browser → crawl4ai → MCP tool → markdown output) and ensure Claude Code users know about the new capability.
Output: A confirmed working crawl_url tool and an updated README with crawl_url documented.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-crawl/02-01-SUMMARY.md
@src/crawl4ai_mcp/server.py
@README.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Smoke test crawl_url with a real crawl</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
Run an end-to-end smoke test of crawl_url using a direct Python invocation that exercises the full browser → crawl4ai → tool pipeline. Do NOT call MCP JSON-RPC directly (the tool should be tested as a Python function with a mocked context, or via a standalone async script).

Write a temporary smoke test script at /tmp/smoke_crawl.py (do not add to the repo):

```python
import asyncio
import logging
import sys
logging.basicConfig(level=logging.WARNING, stream=sys.stderr)

from crawl4ai import AsyncWebCrawler, BrowserConfig
from crawl4ai_mcp.server import _build_run_config, _crawl_with_overrides, _format_crawl_error

async def main():
    browser_cfg = BrowserConfig(headless=True, verbose=False, extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"])
    crawler = AsyncWebCrawler(config=browser_cfg)
    await crawler.start()
    try:
        run_cfg = _build_run_config()
        result = await _crawl_with_overrides(crawler, "https://example.com", run_cfg)
        if not result.success:
            print("FAIL:", _format_crawl_error("https://example.com", result))
            return
        md = result.markdown
        content = (md.fit_markdown or md.raw_markdown) if md else ""
        if not content:
            print("FAIL: empty content returned")
            return
        print(f"PASS: got {len(content)} chars of markdown")
        print("First 200 chars:", content[:200])
    finally:
        await crawler.close()

asyncio.run(main())
```

Run with: `uv run python /tmp/smoke_crawl.py` from /Users/brianpotter/ai_tools/crawl4ai_mcp.

The test PASSES when it prints "PASS: got N chars of markdown" with N > 0 and shows readable content from example.com. If the test FAILS, diagnose the error in server.py (do not modify the smoke script — fix the source).

After confirming pass, delete the smoke script: `rm /tmp/smoke_crawl.py`

Also verify the MCP tool list includes crawl_url by running the JSON-RPC tools/list check:
```bash
echo '{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test","version":"1"}}}
{"jsonrpc":"2.0","id":2,"method":"tools/list","params":{}}' | uv run python -m crawl4ai_mcp.server 2>/dev/null | grep -o '"name":"[^"]*"'
```
Confirm "crawl_url" and "ping" both appear in output.
  </action>
  <verify>
Smoke test script prints "PASS: got N chars of markdown" with N > 100.
tools/list grep shows both "name":"ping" and "name":"crawl_url".
  </verify>
  <done>Smoke test passes with non-empty markdown from example.com; crawl_url appears in MCP tool list alongside ping</done>
</task>

<task type="auto">
  <name>Task 2: Update README with crawl_url documentation</name>
  <files>README.md</files>
  <action>
Update the README to document the new crawl_url tool. Make two changes:

**1. Expand the Available Tools table** (currently lists only `ping`) to include `crawl_url`:

| Tool | Description |
|------|-------------|
| `ping` | Health check — confirms server and browser are running |
| `crawl_url` | Crawl a URL and return clean markdown content. Supports JS rendering, custom headers/cookies, cache control, and CSS content scoping. |

**2. Add a "Usage" section** after the "Available Tools" section (and before "How It Works"). Title it `## Usage` and include a short example block showing how Claude Code would use the tool, plus a quick-reference parameter table:

```
## Usage

Once registered, Claude Code can use the crawl tool in any conversation:

> "Crawl https://docs.python.org/3/library/asyncio.html and summarize the key concepts"
> "Fetch https://example.com with a custom Authorization header"
> "Crawl this JS-heavy page and wait for #content to appear before extracting"

### crawl_url parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `url` | string | required | URL to crawl |
| `cache_mode` | string | `"enabled"` | Cache behavior: `enabled`, `bypass`, `disabled`, `read_only`, `write_only` |
| `css_selector` | string | none | Restrict extraction to elements matching this CSS selector |
| `excluded_selector` | string | none | Exclude elements matching this selector (navbars, footers, etc.) |
| `wait_for` | string | none | Wait before extracting: `"css:#id"` or `"js:() => condition"` |
| `js_code` | string | none | JavaScript to run after page load (scroll, click, etc.) |
| `user_agent` | string | none | Override User-Agent for this request |
| `headers` | object | none | Custom HTTP headers `{"Authorization": "Bearer token"}` |
| `cookies` | array | none | Cookies `[{"name": "s", "value": "x", "domain": "example.com"}]` |
| `page_timeout` | integer | `60` | Page load timeout in seconds |
| `word_count_threshold` | integer | `10` | Min words for a content block to survive filtering |
```

Do NOT change any other sections of the README. Preserve all existing content exactly.
  </action>
  <verify>
`grep -c "crawl_url" README.md` — must return >= 3 (table entry, parameter table heading, at least one usage mention).
`uv run ruff check src/` — still zero errors (README edit shouldn't affect this, but confirm).
  </verify>
  <done>README tool table lists both ping and crawl_url; Usage section with parameter table exists; no existing README content removed</done>
</task>

</tasks>

<verification>
After both tasks:
1. Smoke test passed — example.com returns non-empty fit_markdown via _crawl_with_overrides + _build_run_config
2. `tools/list` JSON-RPC response contains both "ping" and "crawl_url"
3. README has crawl_url in tool table and Usage section with parameter table
4. `uv run ruff check src/` still reports zero errors
</verification>

<success_criteria>
- Real crawl of example.com produces non-empty markdown output (confirms browser + crawl4ai + tool pipeline works end-to-end)
- crawl_url visible in MCP tool list (confirms tool is correctly registered)
- README updated with crawl_url documentation and parameter reference table
- All Phase 2 success criteria from ROADMAP.md are demonstrably met through the smoke test
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-crawl/02-02-SUMMARY.md`
</output>
