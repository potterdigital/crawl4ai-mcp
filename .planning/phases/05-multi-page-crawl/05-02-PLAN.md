---
phase: 05-multi-page-crawl
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/crawl4ai_mcp/server.py
  - tests/test_deep_crawl.py
autonomous: true
requirements:
  - MULTI-02
  - MULTI-04

must_haves:
  truths:
    - "Claude can call deep_crawl with a start URL, max_depth, and max_pages and the crawl stops at those hard limits"
    - "Deep crawl domain scope is agent-configurable: same-domain (default), same-origin, or any"
    - "Agent can filter which links to follow via include_pattern and exclude_pattern"
    - "Each URL is crawled at most once regardless of how many pages link to it (deduplication)"
    - "Results include depth and parent_url metadata for each page"
  artifacts:
    - path: "src/crawl4ai_mcp/server.py"
      provides: "deep_crawl MCP tool with BFS strategy, filter chain, domain scope"
      contains: "async def deep_crawl"
    - path: "tests/test_deep_crawl.py"
      provides: "Unit tests for deep_crawl tool registration and filter chain construction"
  key_links:
    - from: "src/crawl4ai_mcp/server.py"
      to: "crawl4ai.deep_crawling.BFSDeepCrawlStrategy"
      via: "import and per-call instantiation in deep_crawl"
      pattern: "BFSDeepCrawlStrategy"
    - from: "src/crawl4ai_mcp/server.py"
      to: "crawl4ai.deep_crawling.FilterChain"
      via: "filter chain construction from agent params"
      pattern: "FilterChain"
    - from: "src/crawl4ai_mcp/server.py"
      to: "src/crawl4ai_mcp/server.py:_format_multi_results"
      via: "result formatting"
      pattern: "_format_multi_results"
---

<objective>
Implement the `deep_crawl` MCP tool for BFS link-following crawl with agent-configurable depth, page limits, domain scope, and URL pattern filtering.

Purpose: Enables Claude to discover and crawl an entire site or subsection by following links from a seed URL, with full control over depth, breadth, domain boundaries, and URL patterns. This is the most complex tool in Phase 5 — it wraps crawl4ai's BFSDeepCrawlStrategy with agent-friendly parameters.

Output: `deep_crawl` MCP tool in server.py with BFS strategy, filter chain construction, and unit tests.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-multi-page-crawl/05-RESEARCH.md
@src/crawl4ai_mcp/server.py
@src/crawl4ai_mcp/profiles.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add deep_crawl tool with BFS strategy and filter chain</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
**Imports to add at top of server.py:**
```python
from crawl4ai.deep_crawling import (
    BFSDeepCrawlStrategy,
    FilterChain,
    URLPatternFilter,
)
```

Note: Do NOT import `DomainFilter` — the `include_external` parameter on `BFSDeepCrawlStrategy` handles domain scoping natively. The research identified three scope levels:
- `"same-domain"` (default): `include_external=False` — BFS stays within start URL's domain
- `"same-origin"`: `include_external=False` — same behavior (subdomain matching handled by strategy)
- `"any"`: `include_external=True` — follows all links

**Add `deep_crawl` MCP tool with this signature:**
```python
@mcp.tool()
async def deep_crawl(
    url: str,
    max_depth: int = 3,
    max_pages: int = 100,
    scope: str = "same-domain",
    include_pattern: str | None = None,
    exclude_pattern: str | None = None,
    profile: str | None = None,
    cache_mode: str = "enabled",
    css_selector: str | None = None,
    excluded_selector: str | None = None,
    wait_for: str | None = None,
    js_code: str | None = None,
    user_agent: str | None = None,
    page_timeout: int = 60,
    word_count_threshold: int = 10,
    ctx: Context[ServerSession, AppContext] = None,
) -> str:
```

**Tool docstring must:**
- Explain BFS link-following behavior (start URL -> discover links -> crawl them -> repeat to max_depth)
- Document `max_depth` (default 3): how many levels of links to follow from the start URL
- Document `max_pages` (default 100, per user decision "100+"): hard cap on total pages crawled — crawl stops when reached even if more links exist
- Document `scope` parameter: `"same-domain"` (default, stay within start URL's domain), `"any"` (follow all links including external)
- Document `include_pattern` / `exclude_pattern`: glob patterns for filtering which URLs to follow (e.g., `/docs/*` to only follow documentation links)
- Note that each URL is crawled at most once (automatic deduplication)
- Note that results include depth and parent_url metadata
- Warn that large max_pages values take proportionally longer — the agent controls this

**Implementation:**
1. Resolve cache_mode using same `_CACHE_MAP` pattern as crawl_url.
2. Build filter chain from agent params:
   - Create empty `filters` list
   - If `include_pattern` is not None: append `URLPatternFilter(patterns=[include_pattern])`
   - If `exclude_pattern` is not None: append `URLPatternFilter(patterns=[exclude_pattern], reverse=True)`
   - Create `filter_chain = FilterChain(filters=filters) if filters else FilterChain()`
3. Map `scope` to `include_external`:
   - `"same-domain"` or `"same-origin"` -> `include_external=False`
   - `"any"` -> `include_external=True`
   - Unknown value -> log warning, default to `include_external=False`
4. Create `BFSDeepCrawlStrategy` instance (MUST be fresh per call — mutable state):
   ```python
   strategy = BFSDeepCrawlStrategy(
       max_depth=max_depth,
       max_pages=max_pages,
       include_external=include_external,
       filter_chain=filter_chain,
   )
   ```
5. Build `per_call_kwargs` with same sentinel-guard pattern as crawl_url. Include `deep_crawl_strategy=strategy` in per_call_kwargs (this is why Plan 01 adds it to `_PER_CALL_KEYS`).
6. Call `build_run_config(app.profile_manager, profile, **per_call_kwargs)` to get run_cfg.
7. Call `results = await app.crawler.arun(url=url, config=run_cfg)` — when `deep_crawl_strategy` is set, `arun()` returns `List[CrawlResult]` (not a single result). The `DeepCrawlDecorator` intercepts the call.
8. Pass results to `_format_multi_results(results)` (from Plan 01, or if Plan 01 hasn't run yet, define a local version — but Plan 01 is Wave 1 parallel so this helper may or may not exist yet. If `_format_multi_results` doesn't exist when this plan runs, create it inline with the same spec from Plan 01's Task 1 action.)
9. Log: `logger.info("deep_crawl: %s (depth=%d, max_pages=%d, scope=%s)", url, max_depth, max_pages, scope)`

**CRITICAL SAFETY:**
- Do NOT pass `monitor` to any dispatcher. BFS uses `arun_many` internally with the default dispatcher — this is acceptable per research (MemoryAdaptiveDispatcher's rate limiter is reasonable for deep crawling the same domain).
- `verbose=False` is enforced by `build_run_config` — no additional handling needed.
- BFSDeepCrawlStrategy MUST be created fresh per call — never store it in AppContext.
- Do NOT use `_crawl_with_overrides` — deep crawl manages its own multi-URL sessions via the strategy. Skip headers/cookies for deep_crawl in v1 (note in docstring).
  </action>
  <verify>
    `uv run ruff check src/` passes with no errors.
    `deep_crawl` is registered as an MCP tool (grep for `@mcp.tool` before `async def deep_crawl`).
    `BFSDeepCrawlStrategy` and `FilterChain` are imported.
    No `monitor` parameter on any dispatcher.
    No `print()` calls.
  </verify>
  <done>
    deep_crawl tool is defined in server.py with BFSDeepCrawlStrategy, FilterChain-based URL filtering, agent-configurable scope, max_depth/max_pages hard limits, and profile support.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for deep_crawl</name>
  <files>tests/test_deep_crawl.py</files>
  <action>
Create `tests/test_deep_crawl.py` with unit tests following established patterns.

Tests to include:

1. **test_deep_crawl_tool_registered**: Verify `deep_crawl` appears in the MCP server's tool list.

2. **test_bfs_strategy_imports**: Verify that `BFSDeepCrawlStrategy`, `FilterChain`, and `URLPatternFilter` can be imported from `crawl4ai.deep_crawling` (confirms the installed crawl4ai 0.8.0 has these classes).

3. **test_filter_chain_construction_include**: Create a `URLPatternFilter(patterns=["/docs/*"])` and a `FilterChain(filters=[filter])`. Verify the chain was created with the filter in its filters list.

4. **test_filter_chain_construction_exclude**: Create a `URLPatternFilter(patterns=["/internal/*"], reverse=True)` and verify it was created with `reverse=True`.

5. **test_filter_chain_construction_empty**: Create a `FilterChain()` with no filters. Verify it was created successfully (no error).

6. **test_scope_mapping**: Test the scope-to-include_external mapping logic. This can be a simple function test or a parametrized test:
   - `"same-domain"` -> `include_external=False`
   - `"same-origin"` -> `include_external=False`
   - `"any"` -> `include_external=True`

Use imports from `crawl4ai.deep_crawling` directly for the strategy/filter tests (no need to mock these — they are unit-testable without a browser).
  </action>
  <verify>
    `uv run pytest tests/test_deep_crawl.py -v` passes all tests.
    `uv run pytest` passes all tests (no regressions).
  </verify>
  <done>
    6 unit tests pass covering deep_crawl tool registration, BFS strategy imports, filter chain construction (include, exclude, empty), and scope mapping.
  </done>
</task>

</tasks>

<verification>
1. `uv run ruff check src/` — no lint errors
2. `uv run pytest` — all tests pass (existing + new)
3. `deep_crawl` is registered as an MCP tool
4. BFSDeepCrawlStrategy is instantiated per call (not stored in AppContext)
5. FilterChain correctly composes include/exclude URL patterns
6. `max_pages` default is 100 (per user decision "100+")
7. No `monitor` parameter on any dispatcher
8. No `print()` calls in new code
</verification>

<success_criteria>
- deep_crawl MCP tool exists with BFS link-following, configurable depth/pages/scope
- Agent can filter URLs with include_pattern/exclude_pattern glob patterns
- max_pages hard limit stops the crawl when reached
- Results include depth and parent_url metadata
- Profile support works (deep_crawl_strategy passes through build_run_config)
- All tests pass (existing + 6 new)
- No stdout corruption risks
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-page-crawl/05-02-SUMMARY.md`
</output>
