---
phase: 01-foundation
plan: 1
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - .python-version
  - src/crawl4ai_mcp/__init__.py
  - src/crawl4ai_mcp/server.py
  - uv.lock
autonomous: true
requirements:
  - INFRA-01
  - INFRA-02

must_haves:
  truths:
    - "The uv project exists with correct pyproject.toml dependencies and scripts entry"
    - "The server module starts without error via `uv run python -m crawl4ai_mcp.server`"
    - "stdout contains only JSON-RPC protocol frames — no logging, print output, or crawl4ai verbose output reaches stdout"
    - "All logging goes to stderr, confirmed by piping stdout to a JSON parser with stderr suppressed"
  artifacts:
    - path: "pyproject.toml"
      provides: "Project config with mcp[cli]>=1.26.0, crawl4ai>=0.8.0,<0.9.0, scripts entry, ruff T201 lint rule"
      contains: "crawl4ai-mcp = \"crawl4ai_mcp.server:main\""
    - path: ".python-version"
      provides: "Python version pin"
      contains: "3.12"
    - path: "src/crawl4ai_mcp/__init__.py"
      provides: "Package marker (empty)"
    - path: "src/crawl4ai_mcp/server.py"
      provides: "FastMCP server scaffold with stderr logging, stub ping tool, main() entry point"
      exports: ["mcp", "AppContext", "app_lifespan", "main"]
  key_links:
    - from: "src/crawl4ai_mcp/server.py"
      to: "sys.stderr"
      via: "logging.basicConfig(stream=sys.stderr) as FIRST statement before other imports"
      pattern: "logging\\.basicConfig.*stream=sys\\.stderr"
    - from: "pyproject.toml"
      to: "src/crawl4ai_mcp/server.py"
      via: "[project.scripts] crawl4ai-mcp entry point"
      pattern: "crawl4ai-mcp.*crawl4ai_mcp.server:main"
---

<objective>
Initialize the uv project, scaffold the FastMCP server with stderr-only logging, and add a stub ping tool that confirms the server starts and receives tool calls. No crawler behavior yet — just the skeleton that Plan 01-02 will flesh out with the lifespan singleton.

Purpose: Every subsequent phase depends on a clean server foundation. Stdout hygiene established here is permanent and cannot be retrofitted once other tools are in place.
Output: A runnable FastMCP server (stdio) with a `ping` stub tool and zero stdout contamination.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize uv project and write pyproject.toml</name>
  <files>pyproject.toml, .python-version, uv.lock</files>
  <action>
The working directory is /Users/brianpotter/ai_tools/crawl4ai_mcp. The project root already exists. Initialize with uv and configure pyproject.toml.

Run these commands in order:

```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp
echo "3.12" > .python-version
uv init --no-readme 2>/dev/null || true   # may already be initialized — that is fine
```

Then write (or overwrite) pyproject.toml with this exact content:

```toml
[project]
name = "crawl4ai-mcp"
version = "0.1.0"
description = "crawl4ai MCP server for Claude Code"
requires-python = ">=3.12"
dependencies = [
    "mcp[cli]>=1.26.0",
    "crawl4ai>=0.8.0,<0.9.0",
]

[project.scripts]
crawl4ai-mcp = "crawl4ai_mcp.server:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/crawl4ai_mcp"]

[tool.pytest.ini_options]
asyncio_mode = "auto"

[tool.ruff.lint]
select = ["T201"]
```

Then install dependencies and run the crawl4ai browser setup:

```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp
uv add "mcp[cli]>=1.26.0"
uv add "crawl4ai>=0.8.0,<0.9.0"
uv add --dev pytest pytest-asyncio ruff
uv run crawl4ai-setup
uv run crawl4ai-doctor
```

`crawl4ai-setup` installs Playwright's Chromium browser. `crawl4ai-doctor` verifies the installation. Both commands must complete without fatal errors. If `crawl4ai-doctor` reports warnings about optional features, those are acceptable — failures about the browser or async crawler are not.
  </action>
  <verify>
```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp
cat pyproject.toml | grep -E "mcp\[cli\]|crawl4ai|crawl4ai-mcp"
cat .python-version
uv run crawl4ai-doctor
```
pyproject.toml must contain both dependency pins and the scripts entry. `.python-version` must contain `3.12`. crawl4ai-doctor must exit without errors.
  </verify>
  <done>uv project initialized, dependencies installed and locked, Playwright browser installed, crawl4ai-doctor passes.</done>
</task>

<task type="auto">
  <name>Task 2: Scaffold FastMCP server with stderr-only logging and stub ping tool</name>
  <files>src/crawl4ai_mcp/__init__.py, src/crawl4ai_mcp/server.py</files>
  <action>
Create the src/crawl4ai_mcp package and write server.py. The lifespan and AppContext are intentionally minimal here — Plan 01-02 adds the full AsyncWebCrawler singleton. This plan's server just needs to start, respond to tool calls, and produce zero stdout noise.

Create the package directory and empty __init__.py:
```bash
mkdir -p /Users/brianpotter/ai_tools/crawl4ai_mcp/src/crawl4ai_mcp
touch /Users/brianpotter/ai_tools/crawl4ai_mcp/src/crawl4ai_mcp/__init__.py
```

Write /Users/brianpotter/ai_tools/crawl4ai_mcp/src/crawl4ai_mcp/server.py with this EXACT content (order of statements matters for stderr correctness):

```python
# src/crawl4ai_mcp/server.py
import logging
import sys
from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from dataclasses import dataclass, field

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

# MUST be first: configure all logging to stderr before any library imports emit output.
# Any output to stdout corrupts the MCP stdio JSON-RPC transport.
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    stream=sys.stderr,
)
logger = logging.getLogger(__name__)


@dataclass
class AppContext:
    """Typed context shared across all tool calls via FastMCP lifespan.

    crawler is added in Plan 01-02. This stub exists so server.py imports
    and runs cleanly for Plan 01-01 verification.
    """

    _placeholder: str = field(default="stub", repr=False)


@asynccontextmanager
async def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:
    """FastMCP lifespan: runs at server startup and shutdown.

    AsyncWebCrawler singleton is wired in Plan 01-02.
    """
    logger.info("crawl4ai MCP server starting (stub lifespan — Plan 01-01)")
    try:
        yield AppContext()
    finally:
        logger.info("crawl4ai MCP server shutting down")


mcp = FastMCP("crawl4ai", lifespan=app_lifespan)


@mcp.tool()
async def ping(ctx: Context[ServerSession, AppContext]) -> str:
    """Verify the MCP server is running and accepting tool calls.

    Returns 'ok' when healthy. Returns an error description if the
    server context is unavailable.
    """
    try:
        _app: AppContext = ctx.request_context.lifespan_context
        return "ok"
    except Exception as e:
        logger.error("ping failed: %s", e, exc_info=True)
        return f"error: {e}"


def main() -> None:
    """Entry point for `uv run python -m crawl4ai_mcp.server` and the crawl4ai-mcp script."""
    mcp.run()  # stdio transport is the default — do NOT wrap in asyncio.run()


if __name__ == "__main__":
    main()
```

Key implementation constraints enforced by this code:
- `logging.basicConfig(stream=sys.stderr)` appears before the FastMCP import — this is intentional and MUST NOT be reordered.
- `mcp.run()` is called bare, never inside `asyncio.run()` (double event loop error).
- No `print()` calls anywhere.
- `BrowserConfig` and `CrawlerRunConfig` are not imported yet (no crawl4ai at server import time) — that comes in 01-02.
  </action>
  <verify>
Run the stdout cleanliness smoke test. This command pipes an MCP initialize frame to the server, suppresses stderr, and asserts stdout is valid JSON only:

```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp
echo '{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2024-11-05","capabilities":{},"clientInfo":{"name":"test","version":"1.0"}}}' \
  | timeout 5 uv run python -m crawl4ai_mcp.server 2>/dev/null \
  | python3 -c "import sys,json; data=sys.stdin.read(); lines=[l for l in data.split('\n') if l.strip()]; [json.loads(l) for l in lines]; print('stdout is clean JSON')" \
  || echo "FAIL: stdout corruption or server did not respond"
```

Expected output: `stdout is clean JSON`

Also verify the module imports without error:
```bash
cd /Users/brianpotter/ai_tools/crawl4ai_mcp
uv run python -c "from crawl4ai_mcp.server import mcp, AppContext, main; print('imports ok')"
```
  </verify>
  <done>Server starts cleanly, responds to MCP initialize with valid JSON on stdout only, all logging goes to stderr, module imports succeed without error.</done>
</task>

</tasks>

<verification>
Final verification for Plan 01-01:

1. `cat /Users/brianpotter/ai_tools/crawl4ai_mcp/pyproject.toml` — shows both `mcp[cli]>=1.26.0` and `crawl4ai>=0.8.0,<0.9.0` as dependencies, `crawl4ai-mcp = "crawl4ai_mcp.server:main"` in [project.scripts], and `T201` in [tool.ruff.lint]
2. `cat /Users/brianpotter/ai_tools/crawl4ai_mcp/.python-version` — shows `3.12`
3. `uv run ruff check src/crawl4ai_mcp/server.py --select T201` — no print() violations
4. Stdout cleanliness smoke test passes (in Task 2 verify step)
5. `uv run crawl4ai-doctor` — passes without fatal errors
</verification>

<success_criteria>
- pyproject.toml exists with pinned dependencies, scripts entry, and T201 lint rule
- .python-version contains 3.12
- src/crawl4ai_mcp/__init__.py and server.py exist
- Server module imports and starts without error
- Stdout smoke test confirms zero non-JSON output on stdout during server startup
- crawl4ai-doctor passes (Playwright/Chromium installed)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md` documenting:
- Files created and their purposes
- Exact pyproject.toml dependency pins used (confirm from uv.lock what resolved)
- Result of crawl4ai-doctor
- Result of stdout smoke test
- Any deviations from the plan and why
</output>
