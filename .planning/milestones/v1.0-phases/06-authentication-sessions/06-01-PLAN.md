---
phase: 06-authentication-sessions
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/crawl4ai_mcp/server.py
  - src/crawl4ai_mcp/profiles.py
autonomous: true
requirements:
  - AUTH-01
  - AUTH-02

must_haves:
  truths:
    - "crawl_url with a session_id parameter creates a persistent session that preserves cookies and browser state across calls"
    - "create_session tool returns a session ID that can be passed to subsequent crawl_url calls"
    - "Per-call cookies via the cookies param work alongside session_id (cookies are injected into the session page)"
    - "All active sessions are cleaned up when the server shuts down"
  artifacts:
    - path: "src/crawl4ai_mcp/server.py"
      provides: "AppContext.sessions dict, create_session tool, session_id param on crawl_url, session cleanup in app_lifespan"
      contains: "session_id"
    - path: "src/crawl4ai_mcp/profiles.py"
      provides: "session_id in _PER_CALL_KEYS for profile merge pass-through"
      contains: "session_id"
  key_links:
    - from: "crawl_url"
      to: "build_run_config"
      via: "session_id in per_call_kwargs"
      pattern: "session_id.*per_call_kwargs"
    - from: "app_lifespan finally block"
      to: "crawler.crawler_strategy.kill_session"
      via: "session cleanup loop"
      pattern: "kill_session"
---

<objective>
Add session infrastructure and the `create_session` tool to enable multi-step authenticated crawling workflows.

Purpose: Claude needs to log into a website once and then crawl multiple protected pages without re-authenticating. crawl4ai's native `session_id` on `CrawlerRunConfig` preserves browser state (cookies, localStorage, DOM) across `arun()` calls. This plan wires that capability into the MCP tool interface.

Output: `create_session` MCP tool, `session_id` param on `crawl_url`, session tracking in `AppContext`, session cleanup on shutdown.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-authentication-sessions/06-RESEARCH.md
@src/crawl4ai_mcp/server.py
@src/crawl4ai_mcp/profiles.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add session tracking to AppContext and cleanup to app_lifespan</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
1. Add `import time` and `import uuid` at top of server.py (if not already present).

2. Add a `sessions: dict[str, float]` field to the `AppContext` dataclass. This maps session_id strings to their creation timestamp (seconds since epoch). Update the docstring to describe the sessions field.

3. Update the `yield` line in `app_lifespan` to pass `sessions={}` to AppContext:
   ```python
   yield AppContext(crawler=crawler, profile_manager=profile_manager, sessions={})
   ```

4. In the `app_lifespan` `finally` block, add session cleanup BEFORE `await crawler.close()`:
   ```python
   # Clean up active sessions before closing browser
   for sid in list(app_ctx.sessions.keys()):
       try:
           await crawler.crawler_strategy.kill_session(sid)
       except Exception:
           pass
   ```
   NOTE: The `yield` currently does not capture the AppContext. Refactor to:
   ```python
   app_ctx = AppContext(crawler=crawler, profile_manager=profile_manager, sessions={})
   try:
       yield app_ctx
   finally:
       for sid in list(app_ctx.sessions.keys()):
           try:
               await crawler.crawler_strategy.kill_session(sid)
           except Exception:
               pass
       logger.info("Shutting down browser")
       await crawler.close()
       logger.info("Shutdown complete")
   ```

5. Add `session_id` to `_PER_CALL_KEYS` in `profiles.py` so it passes through `build_run_config` without triggering unknown-key warnings:
   ```python
   _PER_CALL_KEYS: frozenset[str] = frozenset(
       {
           "css_selector",
           "excluded_selector",
           "wait_for",
           "js_code",
           "user_agent",
           "deep_crawl_strategy",
           "session_id",  # AUTH-02: persistent named sessions
       }
   )
   ```
  </action>
  <verify>
    `uv run ruff check src/` passes (no print() calls, no import errors).
    `uv run pytest` passes (existing tests still work with new AppContext field).
  </verify>
  <done>
    AppContext has a `sessions` dict field. app_lifespan initializes it as empty and cleans up all active sessions in the finally block before closing the browser. `session_id` is in `_PER_CALL_KEYS`.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add session_id param to crawl_url and create create_session tool</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
1. Add `session_id: str | None = None` parameter to `crawl_url` after `profile`. Add docstring entry:
   ```
   session_id: Optional session name for persistent browser state. When provided,
       the crawl reuses the same browser page across calls — cookies, localStorage,
       and DOM state persist. First call with a new session_id creates the session
       automatically. Use create_session to set up a session with initial cookies
       before crawling. Sessions have a 30-minute inactivity TTL.
   ```

2. In `crawl_url`'s per_call_kwargs building, add:
   ```python
   if session_id is not None:
       per_call_kwargs["session_id"] = session_id
   ```

3. After the `arun()` call succeeds in `crawl_url`, register the session if session_id was provided and not already tracked:
   ```python
   if session_id and session_id not in app.sessions:
       app.sessions[session_id] = time.time()
   ```

4. Create the `create_session` MCP tool. Place it AFTER `crawl_url` and BEFORE `crawl_many` in server.py:
   ```python
   @mcp.tool()
   async def create_session(
       session_id: str | None = None,
       url: str | None = None,
       cookies: list | None = None,
       headers: dict | None = None,
       ctx: Context[ServerSession, AppContext] = None,
   ) -> str:
       """Create a named browser session for multi-step authenticated workflows.

       The session maintains cookies, localStorage, and browser state across
       multiple crawl_url calls that reference the same session_id.

       Sessions have a 30-minute inactivity TTL — each crawl_url call with the
       session_id resets the timer.

       Args:
           session_id: Name for the session. If not provided, a UUID is generated.
               Use a descriptive name like "github-auth" or "dashboard-session".

           url: Optional URL to navigate to during session creation. Useful for
               login pages where you want to combine session creation with an
               initial crawl. If omitted, the session page is created without
               navigating anywhere.

           cookies: Optional list of cookie dicts to inject into the session.
               Each cookie must have at minimum: name, value, domain.
               These cookies persist in the session for subsequent crawl_url calls.

           headers: Optional dict of HTTP headers to send with the initial request.
               Only applied if url is also provided.
       """
       app: AppContext = ctx.request_context.lifespan_context
       sid = session_id or str(uuid.uuid4())

       if sid in app.sessions:
           return f"Session already exists: {sid}"

       logger.info("create_session: %s (url=%s)", sid, url)

       if url:
           # Create session by crawling a URL (e.g., a login page)
           config = build_run_config(
               app.profile_manager,
               None,
               session_id=sid,
               cache_mode=CacheMode.BYPASS,
           )
           result = await _crawl_with_overrides(app.crawler, url, config, headers, cookies)

           app.sessions[sid] = time.time()

           if not result.success:
               return f"Session created: {sid}\n\nWarning: initial crawl failed:\n{_format_crawl_error(url, result)}"

           md = result.markdown
           content = (md.fit_markdown or md.raw_markdown) if md else ""
           return f"Session created: {sid}\n\nInitial page content:\n{content}"
       else:
           # Create session page without navigating
           if cookies:
               # Need to do a minimal crawl to inject cookies via hooks
               config = build_run_config(
                   app.profile_manager,
                   None,
                   session_id=sid,
                   cache_mode=CacheMode.BYPASS,
               )
               # Use about:blank as a no-op navigation target for cookie injection
               await _crawl_with_overrides(app.crawler, "about:blank", config, None, cookies)
           app.sessions[sid] = time.time()
           return f"Session created: {sid}"
   ```

   CRITICAL: All CrawlerRunConfig instances MUST go through `build_run_config` which forces `verbose=False`. Never construct CrawlerRunConfig directly.

5. Ensure the `CacheMode` import already exists (it should from Phase 2).
  </action>
  <verify>
    `uv run ruff check src/` passes.
    `uv run pytest` passes.
    Verify `create_session` appears in `grep -n "def create_session" src/crawl4ai_mcp/server.py`.
    Verify `session_id` appears in crawl_url signature: `grep "session_id" src/crawl4ai_mcp/server.py`.
  </verify>
  <done>
    crawl_url accepts an optional session_id param that creates/reuses a named session. create_session tool exists for explicit session creation with optional URL navigation and cookie injection. Sessions are tracked in AppContext.sessions.
  </done>
</task>

</tasks>

<verification>
1. `uv run ruff check src/` — no lint errors, no print() calls (T201)
2. `uv run pytest` — all existing tests pass
3. `grep -c "session_id" src/crawl4ai_mcp/server.py` — session_id referenced in crawl_url and create_session
4. `grep -c "sessions" src/crawl4ai_mcp/server.py` — sessions dict in AppContext, lifespan, and tools
5. `grep "session_id" src/crawl4ai_mcp/profiles.py` — session_id in _PER_CALL_KEYS
</verification>

<success_criteria>
- AppContext has a sessions dict field initialized as empty
- app_lifespan cleans up all active sessions before closing the browser
- crawl_url has a session_id param that passes through to CrawlerRunConfig
- create_session tool exists and can create sessions with optional URL and cookies
- session_id is in _PER_CALL_KEYS so it passes through build_run_config
- All existing tests pass
- No print() calls or stdout corruption
</success_criteria>

<output>
After completion, create `.planning/phases/06-authentication-sessions/06-01-SUMMARY.md`
</output>
