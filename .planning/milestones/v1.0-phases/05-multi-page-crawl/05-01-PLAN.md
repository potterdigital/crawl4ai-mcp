---
phase: 05-multi-page-crawl
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/crawl4ai_mcp/server.py
  - src/crawl4ai_mcp/profiles.py
  - tests/test_crawl_many.py
autonomous: true
requirements:
  - MULTI-01
  - MULTI-04

must_haves:
  truths:
    - "Claude can pass a list of URLs to crawl_many and receive all results concurrently"
    - "crawl_many returns both successes and failures — never discards successful results because of individual URL errors"
    - "crawl_many respects profile selection and per-call overrides (same as crawl_url)"
    - "Agent can control concurrency via max_concurrent parameter"
  artifacts:
    - path: "src/crawl4ai_mcp/server.py"
      provides: "crawl_many MCP tool and _format_multi_results helper"
      contains: "async def crawl_many"
    - path: "src/crawl4ai_mcp/profiles.py"
      provides: "deep_crawl_strategy added to _PER_CALL_KEYS"
      contains: "deep_crawl_strategy"
    - path: "tests/test_crawl_many.py"
      provides: "Unit tests for crawl_many tool registration and parameter handling"
  key_links:
    - from: "src/crawl4ai_mcp/server.py"
      to: "crawl4ai.async_dispatcher.SemaphoreDispatcher"
      via: "import and instantiation in crawl_many"
      pattern: "SemaphoreDispatcher"
    - from: "src/crawl4ai_mcp/server.py"
      to: "src/crawl4ai_mcp/profiles.py"
      via: "build_run_config call in crawl_many"
      pattern: "build_run_config"
---

<objective>
Implement the `crawl_many` MCP tool for parallel batch crawling of multiple URLs, plus a shared `_format_multi_results` helper for formatting multi-URL crawl output.

Purpose: Enables Claude to crawl dozens of URLs concurrently in a single tool call, with agent-configurable concurrency and per-call overrides. This is the foundation for batch operations — `crawl_sitemap` (Plan 03) will reuse the same `arun_many` + `SemaphoreDispatcher` pattern and the shared result formatter.

Output: `crawl_many` MCP tool in server.py, `_format_multi_results` helper, `_PER_CALL_KEYS` update in profiles.py, and unit tests.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-multi-page-crawl/05-RESEARCH.md
@src/crawl4ai_mcp/server.py
@src/crawl4ai_mcp/profiles.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add crawl_many tool, _format_multi_results helper, and update _PER_CALL_KEYS</name>
  <files>
    src/crawl4ai_mcp/server.py
    src/crawl4ai_mcp/profiles.py
  </files>
  <action>
**profiles.py change:** Add `"deep_crawl_strategy"` to `_PER_CALL_KEYS` frozenset. This is a one-line change that enables Plan 02 (deep_crawl) to pass the strategy object through `build_run_config` without it being stripped as an unknown key. Do NOT add it to `KNOWN_KEYS` (it is per-call only, never in YAML profiles).

**server.py changes:**

1. Add import at top: `from crawl4ai.async_dispatcher import SemaphoreDispatcher`

2. Add `_format_multi_results(results: list, include_content: bool = True) -> str` helper function near `_format_crawl_error`. This formats a list of CrawlResult objects into a structured string:
   - First line: summary count ("Crawled X of Y URLs successfully.")
   - For each success: `## {result.url}\n\n{content}\n` where content is `result.markdown.fit_markdown or result.markdown.raw_markdown` (or empty string if no markdown). Include depth info from `result.metadata.get("depth")` if present (for deep_crawl reuse).
   - For failures: a `## Failed URLs ({count})` section with `- {url}: {error_message}` per failure.
   - Always return both successes AND failures per user decision — never discard successful results.

3. Add `crawl_many` MCP tool with this signature:
   ```python
   @mcp.tool()
   async def crawl_many(
       urls: list[str],
       max_concurrent: int = 10,
       profile: str | None = None,
       cache_mode: str = "enabled",
       css_selector: str | None = None,
       excluded_selector: str | None = None,
       wait_for: str | None = None,
       js_code: str | None = None,
       user_agent: str | None = None,
       page_timeout: int = 60,
       word_count_threshold: int = 10,
       ctx: Context[ServerSession, AppContext] = None,
   ) -> str:
   ```

   Tool docstring must:
   - Make clear that URLs are crawled concurrently, not sequentially
   - Explain that `max_concurrent` controls parallelism (default 10)
   - Note that per-call params apply to ALL URLs in the batch
   - State that individual URL failures never fail the entire batch
   - Mention that the tool supports profiles (same as crawl_url)

   Implementation:
   - Resolve cache_mode using same `_CACHE_MAP` pattern as crawl_url
   - Build `per_call_kwargs` dict with same sentinel-guard pattern as crawl_url (only include non-None/non-default params)
   - Call `build_run_config(app.profile_manager, profile, **per_call_kwargs)` to get run_cfg
   - Create `SemaphoreDispatcher(semaphore_count=max_concurrent)` — NO `monitor` parameter (Rich Console -> stdout corruption), NO `rate_limiter` (fast batch crawl by default)
   - Call `await app.crawler.arun_many(urls=urls, config=run_cfg, dispatcher=dispatcher)`
   - Pass results to `_format_multi_results(results)`
   - Do NOT use `_crawl_with_overrides` — arun_many manages its own sessions. Skip headers/cookies params for crawl_many in v1 (document this in the docstring: "Note: per-call headers and cookies are not supported for batch crawls. Use crawl_url for requests requiring custom headers or cookies.")
   - Log the crawl: `logger.info("crawl_many: %d URLs (max_concurrent=%d, profile=%s)", len(urls), max_concurrent, profile)`
  </action>
  <verify>
    `uv run ruff check src/` passes with no errors (especially T201 — no print() calls).
    The `crawl_many` function is registered as an MCP tool (verify via grep for `@mcp.tool` before `async def crawl_many`).
    `_format_multi_results` is defined and used by `crawl_many`.
    `"deep_crawl_strategy"` appears in `_PER_CALL_KEYS` in profiles.py.
  </verify>
  <done>
    crawl_many tool is defined in server.py with SemaphoreDispatcher concurrency control, profile support, and _format_multi_results formatting. profiles.py has deep_crawl_strategy in _PER_CALL_KEYS.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for crawl_many</name>
  <files>tests/test_crawl_many.py</files>
  <action>
Create `tests/test_crawl_many.py` with unit tests following the established test patterns from existing test files.

Tests to include:

1. **test_crawl_many_tool_registered**: Verify `crawl_many` appears in the MCP server's tool list (same pattern as existing tool registration tests).

2. **test_format_multi_results_success**: Test `_format_multi_results` with a list containing mock successful CrawlResult objects. Verify output contains "Crawled X of Y URLs successfully", URL headers, and markdown content.

3. **test_format_multi_results_mixed**: Test `_format_multi_results` with a mix of successful and failed CrawlResult mocks. Verify output includes BOTH successes and failures sections — never discards successes.

4. **test_format_multi_results_all_failures**: Test `_format_multi_results` with all failed results. Verify "Crawled 0 of N URLs successfully" and failure section.

5. **test_format_multi_results_depth_metadata**: Test that when `result.metadata` contains `"depth"` key, the output includes depth info in the URL header (for deep_crawl reuse).

6. **test_deep_crawl_strategy_in_per_call_keys**: Verify `"deep_crawl_strategy"` is in `_PER_CALL_KEYS` from profiles.py.

Use `unittest.mock.MagicMock` to create mock CrawlResult objects with `.success`, `.url`, `.markdown.fit_markdown`, `.markdown.raw_markdown`, `.error_message`, `.metadata` attributes.

Import `_format_multi_results` from `crawl4ai_mcp.server` for direct unit testing.
  </action>
  <verify>
    `uv run pytest tests/test_crawl_many.py -v` passes all tests.
    `uv run pytest` passes all tests (no regressions).
  </verify>
  <done>
    6 unit tests pass covering crawl_many tool registration, _format_multi_results with various result combinations, depth metadata rendering, and _PER_CALL_KEYS update.
  </done>
</task>

</tasks>

<verification>
1. `uv run ruff check src/` — no lint errors
2. `uv run pytest` — all tests pass (existing + new)
3. `crawl_many` is registered as an MCP tool
4. `_format_multi_results` handles success-only, failure-only, and mixed results
5. `deep_crawl_strategy` is in `_PER_CALL_KEYS`
6. No `monitor` parameter on any dispatcher instantiation
7. No `print()` calls in new code
</verification>

<success_criteria>
- crawl_many MCP tool exists with SemaphoreDispatcher concurrency control
- Agent can control parallelism via max_concurrent parameter
- Results always include both successes and failures
- Profile support works (same merge path as crawl_url)
- All tests pass (existing + 6 new)
- No stdout corruption risks (verbose=False, no monitor, no print)
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-page-crawl/05-01-SUMMARY.md`
</output>
