---
phase: 05-multi-page-crawl
plan: 03
type: execute
wave: 2
depends_on:
  - 05-01
files_modified:
  - src/crawl4ai_mcp/server.py
  - tests/test_crawl_sitemap.py
autonomous: true
requirements:
  - MULTI-03
  - MULTI-04

must_haves:
  truths:
    - "Claude can call crawl_sitemap with a sitemap XML URL and receive crawl results for all discovered URLs"
    - "Sitemap index files are recursively resolved to extract all leaf URLs"
    - "Gzipped sitemaps (.xml.gz) are automatically decompressed"
    - "Agent can limit how many sitemap URLs are crawled via max_urls parameter"
    - "Individual URL failures never fail the entire sitemap crawl"
  artifacts:
    - path: "src/crawl4ai_mcp/server.py"
      provides: "crawl_sitemap MCP tool and _fetch_sitemap_urls helper"
      contains: "async def crawl_sitemap"
    - path: "tests/test_crawl_sitemap.py"
      provides: "Unit tests for sitemap XML parsing and crawl_sitemap tool registration"
  key_links:
    - from: "src/crawl4ai_mcp/server.py:_fetch_sitemap_urls"
      to: "httpx.AsyncClient"
      via: "HTTP GET to fetch sitemap XML"
      pattern: "httpx.AsyncClient"
    - from: "src/crawl4ai_mcp/server.py:_fetch_sitemap_urls"
      to: "xml.etree.ElementTree"
      via: "XML parsing to extract <loc> URLs"
      pattern: "ET.fromstring"
    - from: "src/crawl4ai_mcp/server.py:crawl_sitemap"
      to: "src/crawl4ai_mcp/server.py:_format_multi_results"
      via: "result formatting reuse from crawl_many"
      pattern: "_format_multi_results"
---

<objective>
Implement the `crawl_sitemap` MCP tool for parsing XML sitemaps and crawling all discovered URLs, plus the `_fetch_sitemap_urls` helper for sitemap XML parsing.

Purpose: Enables Claude to crawl an entire site's content inventory by providing its sitemap URL. The tool fetches the sitemap XML (using httpx, not the browser — sitemaps are plain XML), extracts all `<loc>` URLs, and crawls them concurrently via `arun_many`. Supports sitemap index files (recursive resolution) and gzipped sitemaps.

Output: `crawl_sitemap` MCP tool, `_fetch_sitemap_urls` async helper, and unit tests.
</objective>

<execution_context>
@/Users/brianpotter/.claude/get-shit-done/workflows/execute-plan.md
@/Users/brianpotter/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-multi-page-crawl/05-RESEARCH.md
@.planning/phases/05-multi-page-crawl/05-01-SUMMARY.md
@src/crawl4ai_mcp/server.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add _fetch_sitemap_urls helper and crawl_sitemap tool</name>
  <files>src/crawl4ai_mcp/server.py</files>
  <action>
**Imports to add at top of server.py:**
```python
import gzip
import xml.etree.ElementTree as ET

import httpx
```

Note: `httpx` is already a transitive dependency of crawl4ai (no new package install needed). `gzip` and `xml.etree.ElementTree` are stdlib.

**Add `_fetch_sitemap_urls` async helper function:**

```python
SITEMAP_NS = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}

async def _fetch_sitemap_urls(sitemap_url: str) -> list[str]:
    """Fetch and parse a sitemap XML, returning all <loc> URLs.

    Handles:
    - Regular sitemaps (<urlset> with <url><loc>)
    - Sitemap indexes (<sitemapindex> with <sitemap><loc>) — recursively resolved
    - Gzipped sitemaps (.xml.gz) — automatically decompressed
    - Sitemaps with or without XML namespace prefix
    """
    async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:
        resp = await client.get(sitemap_url)
        resp.raise_for_status()

    content = resp.content
    if sitemap_url.endswith(".gz"):
        content = gzip.decompress(content)

    root = ET.fromstring(content)

    # Check if this is a sitemap index
    sub_sitemaps = root.findall("sm:sitemap/sm:loc", SITEMAP_NS)
    if sub_sitemaps:
        urls = []
        for loc_elem in sub_sitemaps:
            sub_urls = await _fetch_sitemap_urls(loc_elem.text.strip())
            urls.extend(sub_urls)
        return urls

    # Regular sitemap — extract <url><loc> entries
    # Try with namespace first, then without (some sitemaps omit namespace)
    locs = root.findall("sm:url/sm:loc", SITEMAP_NS)
    if not locs:
        locs = root.findall("url/loc")
    return [loc.text.strip() for loc in locs if loc.text]
```

Place this near `_format_crawl_error` and `_format_multi_results` (helper functions section).

**Add `crawl_sitemap` MCP tool:**

```python
@mcp.tool()
async def crawl_sitemap(
    sitemap_url: str,
    max_urls: int = 500,
    max_concurrent: int = 10,
    profile: str | None = None,
    cache_mode: str = "enabled",
    css_selector: str | None = None,
    excluded_selector: str | None = None,
    wait_for: str | None = None,
    js_code: str | None = None,
    user_agent: str | None = None,
    page_timeout: int = 60,
    word_count_threshold: int = 10,
    ctx: Context[ServerSession, AppContext] = None,
) -> str:
```

**Tool docstring must:**
- Explain that this tool fetches a sitemap XML URL, extracts all page URLs, and crawls them concurrently
- Document `max_urls` (default 500): maximum number of sitemap URLs to crawl — large sitemaps can have 50,000+ URLs, this prevents runaway crawls
- Document `max_concurrent` (default 10): parallel crawl concurrency
- Note that sitemap index files are automatically resolved (recursive)
- Note that gzipped sitemaps (.xml.gz) are supported
- Note that the sitemap itself is fetched via HTTP (not the browser) since sitemaps are plain XML
- Note that individual URL failures never fail the entire batch
- Note: per-call headers and cookies are not supported for sitemap crawls (same as crawl_many)

**Implementation:**
1. Log: `logger.info("crawl_sitemap: %s (max_urls=%d, max_concurrent=%d)", sitemap_url, max_urls, max_concurrent)`
2. Try to fetch sitemap URLs via `_fetch_sitemap_urls(sitemap_url)`. Wrap in try/except to catch httpx errors and XML parsing errors — return a structured error string if fetching/parsing fails (e.g., "Sitemap fetch failed\nURL: {sitemap_url}\nError: {str(e)}").
3. If no URLs found, return "No URLs found in sitemap\nURL: {sitemap_url}\nThe sitemap may be empty or use an unsupported format."
4. If `len(urls) > max_urls`, truncate to `urls[:max_urls]` and note the truncation in the output.
5. Resolve cache_mode, build per_call_kwargs, call `build_run_config` — same pattern as crawl_many.
6. Create `SemaphoreDispatcher(semaphore_count=max_concurrent)` — NO monitor.
7. Call `await app.crawler.arun_many(urls=urls, config=run_cfg, dispatcher=dispatcher)`.
8. Format with `_format_multi_results(results)`.
9. If URLs were truncated, prepend a note: "Note: Sitemap contained {total} URLs; crawled first {max_urls} (max_urls limit).\n\n"
  </action>
  <verify>
    `uv run ruff check src/` passes with no errors.
    `crawl_sitemap` is registered as an MCP tool.
    `_fetch_sitemap_urls` is defined.
    `SITEMAP_NS` constant is defined.
    Imports include `gzip`, `xml.etree.ElementTree as ET`, and `httpx`.
    No `print()` calls.
  </verify>
  <done>
    crawl_sitemap tool is defined in server.py with sitemap XML parsing, recursive sitemap index resolution, gzip support, max_urls limit, and concurrent crawling via arun_many.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for sitemap parsing and crawl_sitemap</name>
  <files>tests/test_crawl_sitemap.py</files>
  <action>
Create `tests/test_crawl_sitemap.py` with unit tests. The sitemap parser is pure logic (XML parsing) and highly testable without a browser.

Tests to include:

1. **test_crawl_sitemap_tool_registered**: Verify `crawl_sitemap` appears in the MCP server's tool list.

2. **test_fetch_sitemap_urls_regular**: Test `_fetch_sitemap_urls` with a mocked httpx response containing a valid sitemap XML with namespace:
   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
     <url><loc>https://example.com/page1</loc></url>
     <url><loc>https://example.com/page2</loc></url>
   </urlset>
   ```
   Mock `httpx.AsyncClient.get` to return this XML. Verify the function returns `["https://example.com/page1", "https://example.com/page2"]`.

3. **test_fetch_sitemap_urls_no_namespace**: Test with a sitemap XML that omits the namespace declaration:
   ```xml
   <?xml version="1.0"?>
   <urlset>
     <url><loc>https://example.com/page1</loc></url>
   </urlset>
   ```
   Verify it still extracts the URL (fallback to non-namespace query).

4. **test_fetch_sitemap_urls_index**: Test with a sitemap index XML that references sub-sitemaps:
   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
     <sitemap><loc>https://example.com/sitemap1.xml</loc></sitemap>
   </sitemapindex>
   ```
   Mock the recursive call to return URLs from the sub-sitemap. Verify the function resolves recursively.

5. **test_fetch_sitemap_urls_empty**: Test with an empty sitemap (valid XML, no `<url>` elements). Verify it returns an empty list.

6. **test_sitemap_ns_constant**: Verify `SITEMAP_NS` dict contains the standard sitemap namespace `http://www.sitemaps.org/schemas/sitemap/0.9`.

Use `unittest.mock.AsyncMock` and `unittest.mock.patch` to mock `httpx.AsyncClient`. Import `_fetch_sitemap_urls` and `SITEMAP_NS` from `crawl4ai_mcp.server` for direct testing.
  </action>
  <verify>
    `uv run pytest tests/test_crawl_sitemap.py -v` passes all tests.
    `uv run pytest` passes all tests (no regressions).
  </verify>
  <done>
    6 unit tests pass covering crawl_sitemap tool registration, sitemap XML parsing (with/without namespace, index files, empty sitemaps), and SITEMAP_NS constant.
  </done>
</task>

</tasks>

<verification>
1. `uv run ruff check src/` — no lint errors
2. `uv run pytest` — all tests pass (existing + new)
3. `crawl_sitemap` is registered as an MCP tool
4. `_fetch_sitemap_urls` correctly handles: regular sitemaps, sitemap indexes, no-namespace sitemaps
5. Gzip decompression path exists (`.endswith(".gz")` check)
6. `max_urls` truncation works and outputs a note about truncation
7. No browser used for sitemap fetching (httpx only)
8. No stdout corruption risks
</verification>

<success_criteria>
- crawl_sitemap MCP tool exists with httpx-based sitemap fetching and arun_many crawling
- _fetch_sitemap_urls handles regular sitemaps, sitemap indexes, gzipped sitemaps, and namespace variations
- Agent can limit sitemap crawl size via max_urls parameter
- Individual URL failures never fail the entire crawl
- All tests pass (existing + 6 new)
- No stdout corruption risks
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-page-crawl/05-03-SUMMARY.md`
</output>
